<features> ::= <convolution> | <convolution> | <pooling> | <pooling> | <dropout> | <batch_norm>
<convolution> ::= layer:conv [out_channels,int_power2,1,4,9] [kernel_size,int,1,2,5] [stride,int,1,1,3] <padding> <activation_function> <bias>
<batch_norm> ::= layer:batch_norm
<pooling> ::= <pool_type> [kernel_size,int,1,2,5] [stride,int,1,1,3] <padding>
<pool_type> ::= layer:pool_avg | layer:pool_max
<padding> ::= padding:same | padding:valid
<dropout> ::= layer:dropout [rate,float,1,0,0.7]
<activation_function> ::= act:linear | act:relu | act:sigmoid
<bias> ::= bias:True | bias:False
<learning> ::= <lars> [batch_size,int_power2,1,5,12] epochs:50 | <gradient_descent> [batch_size,int_power2,1,5,12] epochs:50 | <rmsprop> [batch_size,int_power2,1,5,12] epochs:50 | <adam> [batch_size,int_power2,1,5,12] epochs:50
<gradient_descent> ::= learning:gradient_descent [lr,float,1,0.0001,0.1] [momentum,float,1,0.68,0.99] [weight_decay,float,1,0.000001,0.001] <nesterov>
<nesterov> ::= nesterov:True | nesterov:False
<adam> ::= learning:adam [lr,float,1,0.0001,0.1] [beta1,float,1,0.5,0.9999] [beta2,float,1,0.5,0.9999] [weight_decay,float,1,0.000001,0.001]
<rmsprop> ::= learning:rmsprop [lr,float,1,0.0001,0.1] [alpha,float,1,0.5,1] [weight_decay,float,1,0.000001,0.001]
<lars> ::= learning:lars [lr_weights,float,1,0.05,0.35] [lr_biases,float,1,0.001,0.01] [momentum,float,1,0.7,0.9] [weight_decay,float,1,0.0000001,0.00001]
<pretext> ::= pretext:bt [lamb,inv_power2,1,5,12]
<projector> ::= <proj_fully_connected> | <proj_batch_norm>
<proj_fully_connected> ::= projector_layer:fc <activation_function> [out_features,int_power2,1,5,12] <bias>
<proj_batch_norm> ::= projector_layer:batch_norm <activation_function>
<identity> ::= projector_layer:identity